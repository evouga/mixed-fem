\documentclass[letterpaper,12pt]{article}
\usepackage[margin=0.65in]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{mathtools}
%\usepackage{algorithm,algpseudocode}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{subcaption}
\usepackage[section]{placeins}
\usepackage{algorithm2e}


\theoremstyle{remark}
\newtheorem{claim}{Claim}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\ceiling{\lceil}{\rceil}

\usepackage{xcolor}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{violet}{rgb}{0.34, 0.02, 0.54}

\newcommand{\danny}[1]{\textcolor{violet}{\textbf{#1}}}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    numbers=left,
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=1
}
 
\lstset{style=mystyle}


\newcommand{\V}{\mathcal{V}}
\newcommand{\HO}{ {H^1(\Omega)} }
\newcommand{\LO}{ {L^2(\Omega)} }
\newcommand{\LG}{ {L^2(\Gamma)} }
\newcommand{\LGin}{ {L^2(\Gin)} }
\newcommand{\LGout}{ {L^2(\Gout)} }
\newcommand{\R}{\mathbb{R}}
\newcommand{\Th}{\mathcal{T}}
\newcommand{\X}{\bar{\mathbf{x}}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\C}{\mathbf{c}}
\newcommand{\s}{\mathbf{s}}
\newcommand{\la}{\mathbf{\lambda}}
\newcommand{\dx}{\delta \x}
\newcommand{\ds}{\delta \s}
\newcommand{\dl}{\delta \la}
\newcommand{\fext}{\mathbf{f}_\text{ext}}
\DeclareMathOperator*{\argmin}{arg\,min} 
\newcommand{\El}{E_\Lambda}
\newcommand{\Hx}{ {H_\x} }
\newcommand{\Hs}{ {H_\s} }
\newcommand{\Hsinv}{H_\s^{-1}}
\newcommand{\Hsk}{ {H_{\s_K}} }
\newcommand{\Hskinv}{H_{\s_K}^{-1}}

\newcommand{\gx}{ {\mathbf{g}_\x} }
\newcommand{\gs}{ {\mathbf{g}_\s} }


\RestyleAlgo{ruled}
\LinesNumbered

\begin{document}


\title{Mixed FEM stuff}
\date{}
\maketitle

\input{setup.tex}
\input{derivs.tex}
%\input{system_quad.tex}

\section{Energy Proxy}
Let $c_K(\x,\s) = W_K \s_K - J_K \x$ where $W_K$ is a function of $\x$. This means that $\El^K(\x,\s,\la) = \la_K \cdot c_K(\x,\s)$. We then have a quadratic approximation of the entire energy that takes the form

\begin{align*}
E(\x,\s,\la) \approx \tilde{E}(\x,\s,\la) &= E_M(\x^k) + E_\Psi(\s^k) - \El(\x^k,\s^k,\la^k) \\
&+ 
(\x - \x^k)^T \left(
	\frac{\partial E_M}{\partial \x} 
 -  \frac{\partial \C}{\partial \x} \la^k \right) & \\
&+ (\s - \s^k)^T \left(
	\frac{\partial E_\Psi}{\partial \s} 
 -  \frac{\partial \C}{\partial \s} \la^k \right)  & \\
&- (\la - \la^k)^T \C (\x^k, \s^k) & \left(\C (\x^k,\s^k) = \left. \frac{\partial \El}{\partial \la} \right|_{\x^k,\s^k,\la^k}\right) \\
& - (\x - \x^k)^T \left(
  \frac{\partial}{\partial \x} \left(\frac{\partial \C}{\partial \s} \la^k\right) \right)(\s - \s^k) & \\
& - (\x - \x^k)^T \left(
  \frac{\partial \C }{\partial \x} \right)(\la - \la^k) 
- (\s - \s^k)^T \left(
  \frac{\partial \C }{\partial \s} \right)(\la - \la^k) & \\
& + \frac{1}{2} (\s- \s^k)^T \left(\frac{\partial^2 E_\Psi}{\partial \s^2} \right)(\s - \s^k) & \\
& + \frac{1}{2}(\x- \x^k)^T \left(\frac{\partial^2 E_M}{\partial \x^2} -
\frac{\partial}{\partial \x} \left(\frac{\partial \C}{\partial \x} \la^k\right)
\right)(\x - \x^k) & \\
\end{align*}
where all derivatives are evaluated at $\x^k, \s^k$, and $\la^k$. Next we cancel out some terms and also take the second derivative of $\C$ with respect to $\x$ to be zero and get
\begin{equation}
\begin{split}
\tilde{E}(\x,\s,\la) &=  E_M(\x^k) + E_\Psi(\s^k) + 
(\x - \x^k)^T \left(\frac{\partial E_M}{\partial \x} \right)  \\
&+ (\s - \s^k)^T \left(\frac{\partial E_\Psi}{\partial \s} \right)   
 - \la^T \C (\x^k, \s^k) \\
& - (\x - \x^k)^T \left(
  \frac{\partial}{\partial \x} \left(\frac{\partial \C}{\partial \s} \la^k\right) \right)(\s - \s^k) \\
& - (\x - \x^k)^T \left(
  \frac{\partial \C }{\partial \x} \right)(\la) 
- (\s - \s^k)^T \left(
  \frac{\partial \C }{\partial \s} \right)(\la) \\
& + \frac{1}{2}(\s- \s^k)^T \left(\frac{\partial^2 E_\Psi}{\partial \s^2} \right)(\s - \s^k) \\
& + \frac{1}{2}(\x- \x^k)^T \left(\frac{\partial^2 E_M}{\partial \x^2}
\right)(\x - \x^k), \\
\end{split}
\end{equation}
and now substituting in $\dx = \x - \x^k$ and $\ds = \s - \s^k$:
\begin{equation}
\begin{split}
\tilde{E}(\x,\s,\la) &=  E_M(\x^k) + E_\Psi(\s^k) + 
\dx^T \left(\frac{\partial E_M}{\partial \x} \right) 
+ \ds^T \left(\frac{\partial E_\Psi}{\partial \s} \right)   
 - \la^T \C (\x^k, \s^k) \\
& - \dx^T \left(
  \frac{\partial}{\partial \x} \left(\frac{\partial \C}{\partial \s} \la^t\right) \right)\ds
- \left( \dx^T 
  \frac{\partial \C }{\partial \x}
+  \ds^T  
  \frac{\partial \C }{\partial \s} \right) \la \\
&+ \frac{1}{2}\ds^T \left(\frac{\partial^2 E_\Psi}{\partial \s^2} \right)\ds
+ \frac{1}{2} \dx^T \left(\frac{\partial^2 E_M}{\partial \x^2}
\right)\dx. \\
&\\
\end{split}
\end{equation}
Finally we substitute some of the gradient terms we've previously defined:
\begin{equation}
\begin{split}
\tilde{E}(\x,\s,\la) &=  E_M(\x^k) + E_\Psi(\s^k) + 
\dx^T \gx
+ \ds^T \gs
 - \la^T \C (\x^k, \s^k) \\
& - \dx^T \left(
  \frac{\partial}{\partial \x} \left(\frac{\partial \C}{\partial \s} \la^k\right) \right)\ds
- \left( \dx^T 
  \frac{\partial \C }{\partial \x}
+  \ds^T  
  \frac{\partial \C }{\partial \s} \right) \la \\
& + \frac{1}{2}\ds^T \Hs \ds
+ \frac{1}{2} \dx^T \Hx \dx. \\
\end{split}
\end{equation}

\section{System of Equations}
Now to setup our system of equations, we differentiate our energy with respect to the $\delta$-variables and set equal to zero. We switch back to vector notation and make $K$ a subscript to simplify the notation.


\subsection{Equation for $\dx$}
Last we differentiate $\tilde{E}$ with respect to $\dx$ (we omit the $\frac{\partial^2 \C_K}{ \partial \x^2}$ term for now):

\begin{equation}
\frac{\partial \tilde{E}}{\partial \dx} = \Hx\dx + \gx 
- \sum_{K \in \Th} \left(
J_K^T(\hat{W}_K^{k}\cdot \la_K^k) \ds_K +
J_K^T(\hat{W}_K^{k}\cdot \s_K^k - I)\la_K
\right)V_K
\end{equation}


\subsection{Equation for $\ds$}
We start by differentiating $\tilde{E}$ with respect to $\ds$:
\begin{equation}
\frac{\partial \tilde{E}^K}{\partial \ds_K} =  \gs_K + \Hsk \ds_K -
{W_K^k}^T \la_K - (\hat{W}_K^{k}\cdot \la_K^k)^T J_K \dx
\end{equation}
where $(\hat{W}_K^{t}\cdot \la_K^t)$ performs a reduction over the middle index of $\hat{W}^K_{ijk}$. Setting this derivative equal to zero and solving for $\ds_K$ gives
\begin{equation}
\ds_K = \Hskinv \left( {W_K^k}^T \la_K + (\hat{W}_K^{k}\cdot \la_K^k)^T J_K \dx - \gs \right).
\end{equation}

\subsection{Equation for $\dl$}
In the same fashion we differentiate $\tilde{E}$ with respect to $\dl$:

\begin{equation}
\begin{split}
\frac{\partial \tilde{E}^K}{\partial \la_K} &= -\C_K (x^k,\s^k)- \left( \dx^T 
  \frac{\partial \C_K }{\partial \x} +  \ds^T \frac{\partial \C_K }{\partial \s} \right) \\
  &= -\C_K (x^k,\s^k) -(\hat{W}_K^{k}\cdot \s_K^k - I)^T J_K \dx - W_K^k \ds\\
\end{split}
\end{equation}
where $(\hat{W}_K^{k}\cdot \s_K^k)$ performs a reduction over the last index of $\hat{W}^K_{ijk}$.


\subsection{System of Equations}
To build the system of equations we define some variables to keep things neat:
\begin{align}
F &= - \sum_{K \in \Th} {P_K^\s}^T \left((\hat{W}_K^{k}\cdot \la_K^k)^T J_K^w \right) \\
G &= J^w - \sum_{K \in \Th} {P_K^\la}^T \left((\hat{W}_K^{k}\cdot \s_K^k)^T J_K^w \right) \\
D &= -\sum_{K \in \Th} {P_K^\la}^T  W_K^k P_K^\s
\end{align}
where $J^w$ is the volume weighted Jacobian, and $P_K^\la \in \R^{9 \times 9n_e}$ and $P_K^\s \in \R^{6 \times 6n_e}$ are selection matrics for the $K$-th set of Lagrange multipliers and deformation variables, respectively. Additionally $\Hs$ is the block diagonal Hessian for $s^k$. We form the system by setting each of the derivatives with respect to the $\delta$-variables equal to zero and get

\begin{equation}
\begin{pmatrix}
\Hx & F^T & G^T \\
F & \Hs & D^T\\
G & D & 0
\end{pmatrix}
\begin{pmatrix}
\dx \\
\ds \\
\la
\end{pmatrix} =
\begin{pmatrix}
-\gx \\
-\gs \\
\C (\x^k, \s^k)
\end{pmatrix}.
\end{equation}

Next, to get this thing in a KKT-like form we substitute the solution for the $\ds$ into the other equations. So we have
\begin{equation}
\ds = \Hsinv \left(-\gs - D^T \la - F\dx \right),
\end{equation}
and sorry cause it's about to get a little ugly, but for the first set of equations we have
\begin{equation*}
\Hx\dx + F^T\Hsinv \left(-\gs - D^T\la - F\dx \right) +G^T \la = -\gx
\end{equation*}
and collect terms to get
\begin{equation}
\left(\Hx - F^T\Hsinv F \right)\dx + (G^T - F^T\Hsinv D^T)\la = 
F^T\Hsinv \gs -\gx.
\end{equation}
Next we substitute the solution for $\ds$ into the equations for $\la$:
\begin{equation*}
G\dx + D\Hsinv \left(-\gs - D^T\la - F\dx\right) = \C (\x^k, \s^k)
\end{equation*}
and collect terms so that we have
\begin{equation}
(G - D\Hsinv F)\dx - D\Hsinv D^T\la = \C (\x^k, \s^k) + D\Hsinv\gs.
\end{equation}
Before moving the equations back into matrix form, we define the following variables:
\begin{align}
\tilde{M} &= (\Hx - F^T\Hs^{-1}F) \\
\tilde{J} &= (G - D\Hsinv F). 
\end{align}

Now our final system of equations for a single Newton step is
\begin{equation}
\begin{pmatrix}
\tilde{M} & \tilde{J}^T \\
\tilde{J} & -D\Hsinv D^T 
\end{pmatrix}
\begin{pmatrix}
\dx \\
\la
\end{pmatrix} =
\begin{pmatrix}
F^T\Hsinv\gs -\gx \\
\C (\x^k, \s^k) + D\Hsinv\gs
\end{pmatrix}.
\end{equation}

\section{The various numerical methods}

%% This declares a command \Comment
%% The argument will be surrounded by /* ... */
\SetKwComment{Comment}{/* }{ */}

\begin{algorithm}[htp]
\caption{Newtons method}\label{alg:one}
\SetKwFunction{algo}{NewtonStep}
\SetKwProg{myalg}{Algorithm}{}{}
\myalg{\algo{$\x^t,\s^t$, \textup{max\_iters}}}{

\SetKwRepeat{Do}{do}{while}

$\x \gets \x^t$\;
$\s \gets \s^t$\;
$\la \gets 0$\;
$\x^0 \gets \x^t$\;
$\s^0 \gets \s^t$\;
$E^0 \gets \text{energy}(\x,\s,\la)$\;

iter $\gets 0$\;
\Do{not converged \textup{or} \textup{iter} $ < $ \textup{max\_iters}}{

	
	\tcp{Compute system LHS and RHS}
	$H,\mathbf{g} = \text{compute\_gradients}(\x,\s,\la)$\;
	\BlankLine
	\tcp{Descent direction}
	$\dx,\la = -H^{-1} \mathbf{g}$\;
	
	$\alpha \gets 1$\;
	\Do{$\text{energy}(\x,\s,\la)$}{
	$\x \gets \x^0 + \alpha \dx$\;
	$\alpha \gets \alpha / 2$\;
	$\s \gets \s^0 + \text{compute\_ds}(\x,\la) > E^0$\;
    }
	$\x^0 \gets \x$\;
	$\s^0 \gets \s$\;
	$E^0 \gets \text{energy}(\x,\s,\la)$\;
	iter $\gets$ iter $ + 1$\;
}
\Return $\x,\s,\la$

}
\end{algorithm}

\end{document}